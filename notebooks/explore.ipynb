{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0048f47f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Thoughts:\n",
    "Age is a potentially high correlator, but many entries have null age. Could see what values correlate to age, and fit a model to predict null ages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0042280",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.base import clone\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from dython.nominal import associations, identify_nominal_columns\n",
    "\n",
    "sys.path.append(\"/home/andrew/PycharmProjects/PyTorch\")\n",
    "from src.kaggle_api import get_dataset\n",
    "from src.estimator_comparison import test_estimators\n",
    "from src.gen import train_test_from_null, get_xy_from_dataframe\n",
    "\n",
    "# Whether to run intensive grid searches (True) or simple fits (False)\n",
    "intensive = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61326fb",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Load in dataset and show the breakdown of null values per column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e15d260",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data_path = get_dataset(\"titanic\")\n",
    "raw_train_data = pd.read_csv(data_path / \"train.csv\")\n",
    "raw_test_data = pd.read_csv(data_path / \"test.csv\")\n",
    "\n",
    "print(raw_train_data.info())\n",
    "print(raw_test_data.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27bb1fe6",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's also see how many rows contain null values, and the breakdown of these per column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec3effc",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Most of the missing values across both training and test datasets come from Cabin and Age. Let's combine the datasets and inspect further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a35399",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "raw_comb_data = pd.concat([raw_train_data, raw_test_data], ignore_index=True)\n",
    "print(raw_comb_data['Cabin'].value_counts())\n",
    "print(raw_comb_data['Age'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d759afc5",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Both factors may correlate to survivability, but with so many missing Cabin entries it makes sense to remove it for now.\n",
    "NOTE: Strip Cabin to letter only and see if there's a correlation/connection between fare/class/cabin/ticket, it may be that lower class cabins are not recorded etc.\n",
    "\n",
    "Before removing the Cabin column, let's inspect some other columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7ae59b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(raw_comb_data[\"Ticket\"].value_counts())\n",
    "raw_comb_data[[\"Ticket\", \"Name\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befc9786",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The Ticket column is very messy and contains duplicates, so it is unlikely that much can be obtained from it, and surely there is no correlation between name and survival!\n",
    "\n",
    "But what about a correlation between name (or more specifically title) and age? This could be a useful predictor, let's try and extract titles using regex:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7d5a6f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#print(list(comb_data[\"Name\"]))\n",
    "raw_comb_data[\"Title\"] = raw_comb_data[\"Name\"].str.extract(r\",\\s?(\\w*).{1}\")\n",
    "raw_comb_data[\"Title\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b03853",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "That worked well! There's just a few outliers to work with. Some of these can be rectified easily by looking at the \"Sex\" column, for example a male with the title Dr or Rev can be called \"Mr\" for our purposes. Others will require a little more thought:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30605a76",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "comb_data = raw_comb_data.copy()\n",
    "comb_data = comb_data.assign(Title=None)\n",
    "\n",
    "replace_male = (comb_data[\"Sex\"] == \"male\") & (~comb_data[\"Title\"].isin([\"Mr\", \"Master\"]))\n",
    "comb_data.loc[replace_male, \"Title\"] = \"Mr\"\n",
    "comb_data.loc[replace_male & (comb_data[\"Age\"] < 18), \"Title\"] = \"Master\"\n",
    "\n",
    "replace_female = (comb_data[\"Sex\"] == \"female\") & (~comb_data[\"Title\"].isin([\"Miss\", \"Mrs\"]))\n",
    "comb_data.loc[replace_female, \"Title\"] = \"Miss\"\n",
    "comb_data.loc[replace_female & (comb_data[\"Age\"] > 18) & (comb_data[\"SibSp\"] | comb_data[\"Parch\"]), \"Title\"] = \"Mrs\"\n",
    "\n",
    "print(comb_data[\"Title\"].value_counts())\n",
    "comb_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06333bd",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Finally, there are a couple of null values left outside the Age column, so let's fill them with reasonable values.\n",
    "\n",
    "TODO: This should be done separately for train and test, build a preprocessing pipeline and apply to both individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780d14e3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "comb_data[\"Fare\"] = comb_data[\"Fare\"].fillna(comb_data[\"Fare\"].mean())\n",
    "comb_data[\"Embarked\"] = comb_data[\"Embarked\"].fillna(comb_data[\"Embarked\"].mode())\n",
    "\n",
    "comb_data.reset_index(drop=True)\n",
    "comb_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89b58f7",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "This is our baseline database to predict both passenger age and survival. It currently contains span both training and test datasets, since we want to use as much data as possible to build the age model.\n",
    "\n",
    "First, let's look at age, dropping unnecessary columns:\n",
    "TODO: Explain why each is dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5540e12",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "age_data = comb_data.copy()\n",
    "age_data = age_data.drop([\"PassengerId\", \"Cabin\", \"Ticket\", \"Name\", \"Survived\", \"Fare\", \"Embarked\", \"Sex\"], axis=1)\n",
    "age_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb07e2e",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We are going to need to encode our Title column to numeric values, let's do that first:\n",
    "NOTE is this appropriate? One-hot encode instead?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9537cd",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "age_data[\"Title\"] = pd.factorize(age_data[\"Title\"])[0]\n",
    "age_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebbfc34",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's look at each feature individually now with respect to age:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e75664",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now we can have a look at how our columns correlate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab54b75",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "age_data.corr(method='pearson')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a776b8a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "It looks like Title does in fact have a high correlation to Age! Let's have a more visual look at this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f328b81a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "assoc_func = lambda data, nom_col: associations(\n",
    "    data,\n",
    "    nominal_columns=nom_col,\n",
    "    numerical_columns=None,\n",
    "    mark_columns=False,\n",
    "    nom_nom_assoc=\"cramer\",\n",
    "    num_num_assoc=\"pearson\",\n",
    "    cramers_v_bias_correction=False,\n",
    "    nan_strategy=\"drop_samples\",\n",
    "    ax=None,\n",
    "    figsize=None,\n",
    "    annot=True,\n",
    "    fmt='.2f',\n",
    "    cmap=None,\n",
    "    sv_color='silver',\n",
    "    cbar=True,\n",
    "    vmax=1.0,\n",
    "    vmin=None,\n",
    "    plot=True,\n",
    "    compute_only=False,\n",
    "    clustering=False,\n",
    "    title=None,\n",
    "    filename=None\n",
    ")\n",
    "\n",
    "correl = assoc_func(age_data, \"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946fd624",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's try again, this time specifying categorical columns. Also, we can now drop the Sex column, since it is fully correlated with Title which gives more information with respect to age."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b1babb",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cat_cols = identify_nominal_columns(age_data)\n",
    "print(cat_cols)\n",
    "\n",
    "nom_features = [\"Pclass\", \"Title\"]\n",
    "assoc_func(age_data, nom_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd874db",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "age_target = \"Age\"\n",
    "features = [c for c in age_data.columns if c != age_target]\n",
    "print(features)\n",
    "\n",
    "for f in features:\n",
    "    g = sns.FacetGrid(age_data, col=f)\n",
    "    g.map_dataframe(sns.histplot, x=age_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbafdd5",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The data becomes very sparse with increasing SibSp and Parch, so let's combine higher numbers\n",
    "NOTE how about combining features overall?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f3f1a2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "age_data[\"SibSp\"] = age_data[\"SibSp\"].clip(upper=3)\n",
    "age_data[\"Parch\"] = age_data[\"Parch\"].clip(upper=2)\n",
    "\n",
    "for f in features:\n",
    "    g = sns.FacetGrid(age_data, col=f)\n",
    "    g.map_dataframe(sns.histplot, x=age_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6e2632",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "For some (ALL?) regressors we have to provide numeric values, so let's convert categorical data to one-hot vectors. We will also return k-1 columns since all 0's in a row will point to the baseline category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e979c767",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# age_data_numerical = pd.get_dummies(age_data, columns=[\"Title\"], drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2883275a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We don't need age to be predicted precisely to the number, rather we could simplify our model if we turned our current continuous age range regression problem into an age band classification problem.\n",
    "\n",
    "To do this, we have to band or \"bin\" our existing age data. We do not want to define these bands arbitrarily, however a reasonable starting point would be to band them in terms of frequency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8a50af",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "bins = 5\n",
    "# age_group_labels = [f\"Group{i}\" for i in range(bins)]\n",
    "age_data[\"Age\"], bin_bounds = pd.qcut(age_data[\"Age\"], q=bins, precision=0, labels=False, retbins=True)\n",
    "\n",
    "age_bins = {i: bin_bounds[i:i+2] for i, k in enumerate(bin_bounds)}\n",
    "print(age_bins)\n",
    "age_data[\"Age\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd4aafb",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Finally, let's split the age dataset into train and test based on which rows do not have age specified. Then we can start making predictions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e279e0f3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_age_data, age_test_data = train_test_from_null(age_data, age_target)\n",
    "X_age, y_age = get_xy_from_dataframe(train_age_data, age_target)\n",
    "age_test_data, _ = get_xy_from_dataframe(age_test_data, age_target)\n",
    "X_age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0252ef1-ae9c-438e-8dca-6ef17cde747e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "candidate_models = [\n",
    "    DecisionTreeClassifier(),\n",
    "    RandomForestClassifier(),\n",
    "    ExtraTreesClassifier(),\n",
    "    GradientBoostingClassifier(),\n",
    "    AdaBoostClassifier()\n",
    "]\n",
    "\n",
    "test_estimators(X_age, y_age, models=candidate_models, type_filter=\"classifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ca6cea",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "clf = DecisionTreeClassifier()\n",
    "\n",
    "hyperparams = {\n",
    "    \"criterion\": ['gini', 'entropy'],\n",
    "    \"max_depth\": range(2, 16),\n",
    "    \"min_samples_split\": range(2, 10),\n",
    "    \"min_samples_leaf\": range(1, 5)\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "chosen_clf = GradientBoostingClassifier(loss=\"log_loss\", criterion=\"friedman_mse\", n_estimators=50)\n",
    "\n",
    "age_clf = clone(chosen_clf)\n",
    "\n",
    "hyperparams = {\n",
    "    \"learning_rate\": [0.01, 0.025, 0.05, 0.075, 0.1],\n",
    "    \"min_samples_split\": np.linspace(0.1, 0.5, 4),\n",
    "    \"min_samples_leaf\": np.linspace(0.1, 0.5, 4),\n",
    "    \"max_depth\": [5, 8],\n",
    "    \"subsample\":[0.6, 0.8, 0.95, 1.0],\n",
    "}\n",
    "\n",
    "if not intensive:\n",
    "    age_clf.fit(X_age, y_age)\n",
    "    age_pred = age_clf.predict(age_test_data)\n",
    "    disp = age_clf\n",
    "else:\n",
    "    age_cv = GridSearchCV(age_clf, param_grid=hyperparams, cv=10, n_jobs=-1, verbose=2)\n",
    "    age_cv.fit(X_age, y_age)\n",
    "    print(\"model score: %.3f\" % age_cv.best_score_)\n",
    "    age_pred = age_cv.predict(age_test_data)\n",
    "    disp = age_cv\n",
    "\n",
    "disp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0e9daa",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "age_test_data = age_test_data.assign(Age=age_pred)\n",
    "\n",
    "all_age_data = pd.concat([train_age_data, age_test_data]).sort_index()\n",
    "all_age_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e05849",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Great! We have our age predictions, now let's go back to our baseline dataset and make another copy for our survival prediction:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d976bc08",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's fill the age column, clip the SibSp and Parch columns again, and re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af07b731",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "survive_data = comb_data.copy()\n",
    "survive_data = survive_data.drop([\"PassengerId\", \"Cabin\", \"Ticket\", \"Name\", \"Sex\"], axis=1)\n",
    "survive_data[\"Title\"] = pd.factorize(survive_data[\"Title\"])[0]\n",
    "survive_data[\"Embarked\"] = pd.factorize(survive_data[\"Embarked\"])[0]\n",
    "survive_data[\"SibSp\"] = survive_data[\"SibSp\"].clip(upper=3)\n",
    "survive_data[\"Parch\"] = survive_data[\"Parch\"].clip(upper=2)\n",
    "survive_data[\"Age\"] = all_age_data[\"Age\"]\n",
    "survive_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbdebb5",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "target = \"Survived\"\n",
    "features = [c for c in survive_data.columns if c != target]\n",
    "\n",
    "train_data, test_data = train_test_from_null(survive_data, target)\n",
    "\n",
    "X_train, y_train = get_xy_from_dataframe(train_data, target)\n",
    "X_test, _ = get_xy_from_dataframe(test_data, target)\n",
    "X_train.info()\n",
    "X_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962d6092",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_estimators(X_train, y_train, models=candidate_models, type_filter=\"classifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486437bf",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Use the same estimator\n",
    "clf = clone(chosen_clf)\n",
    "\n",
    "if not intensive:\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    disp = clf\n",
    "else:\n",
    "    cv = GridSearchCV(clf, param_grid=hyperparams, cv=10, n_jobs=-1, verbose=2)\n",
    "    cv.fit(X_train, y_train)\n",
    "    print(\"model score: %.3f\" % cv.best_score_)\n",
    "    y_pred = cv.predict(X_test)\n",
    "    disp = cv\n",
    "\n",
    "test_data = test_data.copy()\n",
    "test_data[target] = y_pred\n",
    "test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ac68db",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Last but not least, we must save our prediction in csv format, providing only passenger ID and binary survive columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae6bde9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Get correct format\n",
    "test_data.index += 1\n",
    "test_data[target] = test_data[target].astype(int)\n",
    "\n",
    "# Write out\n",
    "test_data.to_csv(data_path / \"initial_prediction.csv\", columns=[target], index=True, index_label=\"PassengerId\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514f6e98",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Great! We can now upload this to Kaggle and see our prediction.\n",
    "\n",
    "Let's review the steps we took here:\n",
    "1. Inspected the data to understand columns that should be dropped, filled, or reduced.\n",
    "2. Combined and copied our dataset train and test datasets for Age predictions.\n",
    "3. Created an additional column Title from Name data, using the Age and Sex columns to replace outliers.\n",
    "4. Filled small number of missing data columns from Embarked and Fare columns.\n",
    "5. Dropped unnecessary columns from the Age dataset.\n",
    "6. Encoded string data from Title column.\n",
    "7. Clipped sparse data from SibSp and Parch columns.\n",
    "8. Binned Age values since a continuous age range is too granular with respect to survivability.\n",
    "9. Predicted missing Age rows, casting them back to the original dataset.\n",
    "10. Carried out a similar process to calculate Survived data.\n",
    "\n",
    "Some notes on this method:\n",
    "- Is the Title column the best we can do? We have a Title for young men (Master), but Miss covers women of all ages! Furthermore, surely being a Miss or Mrs does not determine your fate!\n",
    "- Combining the train and test to predict the Age column is not best practice, this should be done on both sets of data independently.\n",
    "- We went through each data wrangling/featuring engineering step individually, can we automate this?\n",
    "- We chose Age bins based on frequency, how do we know this is the best approach?\n",
    "- Embarked and ... columns are not ordinal features. For decision trees it does not matter, but for numerical estimators they should be transformed to categorical one hot encoders instead.\n",
    "\n",
    "So, how do we move forward? What we would like is something that can carryout all the necessary steps to prepare both our training and testing data, with sufficient modularity to allow these steps to be replaced or updated as we discover more meaningful ways to represent the data. This is where pipelines come in, let's build one using scikit-learn in our next notebook!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pyt] *",
   "language": "python",
   "name": "conda-env-pyt-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
